{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Композиции моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Неустойчивость деревьев \n",
    "Модель должна оставаться устойчивой, если обучающая выборка несильно меняется (например, если выбросить из обучающей выборки 10% данных).\n",
    "\n",
    "<img src = 'images/ensembles1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одиночные деревья - неустойчивы. Разделяющая поверхность сильно меняется при небольшом изменении обучающей выборки. \n",
    "\n",
    "Можно попробовать построить несколько немного разных моделей и усреднить результат (как учет погрешности в физическом эксперименте).\n",
    "### Композиция (ансамбль) моделей\n",
    "Имеется N деревьев $b_1(x), ..., b_N(x)$, объединим их через голосование большинством (majority vote):\n",
    "$$a(x)=argmax_{y \\in Y} \\sum _{n=1} ^N [b_n(x)=y]$$\n",
    "_За какой класс голосует большинство деревьев_\n",
    "\n",
    "<img src = 'images/ensembles2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим два ансамбля деревьев и сравним (по 10 деревьев в каждом):\n",
    "\n",
    "\n",
    "<img src = 'images/ensembles3.png'>\n",
    "Почти нет отличий, ансамбли (усреднение моделей) - устойчивы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация\n",
    "$b_1(x), ..., b_N(x)$ - базовые модели\n",
    "\n",
    "Каждая базовая модель хотя бы немного лучше случайного угадывания\n",
    "\n",
    "Композиция: голосование по большинству (majority vote) $$a_N(x)=argmax_{y \\in Y} \\sum _{n=1} ^N [b_n(x)=y]$$\n",
    "Выдаем тот класс, за который проголосовало больше всего моделей\n",
    "\n",
    "\n",
    "### Регрессия\n",
    "$b_1(x), ... , b_N(x)$ - базовые модели\n",
    "\n",
    "Каждая базовая модель хотя бы немного лучше случайного угадывания\n",
    "\n",
    "Композиция: усреднение\n",
    "\n",
    "$$a_N(x) = \\frac {1}{N} \\sum _{n=1} ^N b_n(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовые модели\n",
    "- Как на одной выборке построить N различных моделей?\n",
    "- Вариант 1: обучить их независимо на разных подвыборках (чтобы каждая модель получалась немного другой, параллельно)\n",
    "- Вариант 2: обучать последовательно для корректировки ошибок (каждая следующая модель корректирует ошибки предыдущей, последовательно)\n",
    "\n",
    "Бустинг - вариант 2\n",
    "\n",
    "\n",
    "Бэггинг - вариант 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (bootstrap aggregating)\n",
    "- базовые модели обучаются независимо\n",
    "- каждая обучается на подмножестве обучающей выборки\n",
    "- подмножество выбирается с помощью бутстрапа\n",
    "\n",
    "### Бутстрап (подвыборка по объектам)\n",
    "- выборка с возвращением\n",
    "- берем $l$ элементов из $X$\n",
    "- Пример: $x_1, x_2, x_3, x_4 -> {x_1, x_2, x_2, x_4}$\n",
    "- В подвыборке будет $l$ объектов, из них около 63.2% уникальных\n",
    "- Если объект входит в выборку несколько раз, то это можно представить как повышение его веса (если ошиблись на объекте, который попал в выборку 2 раза, то в функционале ошибки штраф удваивается)\n",
    "\n",
    "# Случайные подпространства (подвыборка по признакам)\n",
    "- выбираем случайное подмножество признаков\n",
    "- обучаем модель только на них\n",
    "- недостаток: признаки могут быть не равнозначными (могут иметься очень важные признаки, без которых невозможно построить разумную модель, золотой признак)\n",
    "\n",
    "\n",
    "\n",
    "Можно комбинировать разные виды рандомизации\n",
    "- бэггинг (случайная подвыборка)\n",
    "- случайные подпространства (случайное подмножество признаков)\n",
    "\n",
    "Матрица объекты-признаки:\n",
    "\n",
    "\n",
    "<img src='images/ensembles4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Смещение и разброс моделей\n",
    "Разложение ошибки метода обучения на смещение и разброс\n",
    "\n",
    "$$L(\\mu)= E_{x,y}[(y-E[y|x])^2] + E_x[(E_X[\\mu(X)]-E[y|x])^2] + E_x[E_X[\\mu(X) - E_X[(\\mu(X)])^2]]$$\n",
    "\n",
    "$E_{x,y}[(y-E[y|x])^2]$ - шум\n",
    "\n",
    "$E_x[(E_X[\\mu(X)]-E[y|x])^2]$ - смещение\n",
    "\n",
    "$E_x[E_X[\\mu(X) - E_X[(\\mu(X)])^2]]$ - разброс\n",
    "\n",
    "\n",
    "Ошибка модели складывается из трёх компонент:\n",
    "- Шум (noise) - характеристика сложности и противоречивости данных\n",
    "- Смещение (bias) - способность модели приблизить лучшую среди всех возможных моделей (сила модели, насколько она вообще может подогнаться под истинную закономерность)\n",
    "- Разброс (variance) - устойчивость модели к изменениям в обучающей выборке (деревья могут очень сильно меняться даже при небольших изменениях выборки = большой разброс)\n",
    "\n",
    "В итоге нужно искать модели несмещенные (достаточно сложные) и которые имеют низкий разброс (находят общие закономерности в данных, а не просто подгоняются под них)\n",
    "\n",
    "### Линейная модель\n",
    "Линейная регрессия. Черная кривая - лучшая из возможных моделей. Синие - линейные модели на разных подвыборках. \n",
    "\n",
    "<img src='images/ensembles5.png'>\n",
    "\n",
    "Модели очень похожи друг на друга - разброса почти нет. Модель почти не меняется, она устойчивая. Смещение - очень большое, она не угадывает истинную закономерность. \n",
    "\n",
    "### Деревья\n",
    "Фиолетовая кривая - средний прогноз. Синие - деревья глубиной 2 (очень слабые деревья). \n",
    "\n",
    "<img src='images/ensembles6.png'>\n",
    "\n",
    "Смещение ниже, чем у линейной модели. Разброс тоже небольшой (синие кривые не очень сильно отклоняются от фиолетовой). \n",
    "\n",
    "Деревья глубины 3. \n",
    "\n",
    "<img src='images/ensembles7.png'>\n",
    "Смещение еще меньше (фиолетовая кривая еще лучше аппроксимирует реальную зависимость). Разброс увеличился. \n",
    "\n",
    "Деревья глубины 6.\n",
    "\n",
    "<img src='images/ensembles8.png'>\n",
    "\n",
    "При росте глубины дерева снижается смещение, но растет разброс. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бэггинг\n",
    "\n",
    "- Смещение $a_N(x)$ такое же, как у $b_n(x)$ (сложность модели не увеличится -> в бэггинге в качестве базовых моделей нужно брать сложные модели, например, глубокие решающие деревья)\n",
    "\n",
    "- Разброс $a_N(x)$:\n",
    "$$\\frac{1}{N}(разброс b_n(x)) + ковариация(b_n(x), b_m(x)) $$\n",
    "\n",
    "- Если базовые модели независимы, то разброс уменьшается в N раз (независимость = их ошибки независимы. По тому, что первая модель ошиблась на первом объекте, нельзя сказать, что вторая модель тоже ошибется на нем)\n",
    "\n",
    "\n",
    "- Чем более похожи выходы базовых моделей, тем меньше эффект от построения композиции.\n",
    "\n",
    "\n",
    "Следовательно, базовые модели должны быть сложными и как можно более независимыми (непохожими).\n",
    "\n",
    "\n",
    "<img src='images/ensembles9.png'>\n",
    "\n",
    "Каждая синяя кривая - бэггинг и усреднение 10 глубоких деревьев. \n",
    "\n",
    "Синие кривые очень близко к фиолетовой (среднее). Значит разброс уменьшился. Бэггинг позволяет уменьшить разброс. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайный лес\n",
    "\n",
    "Вспомним алгоритм обучения решающего дерева. Жадный, рекурсивный алгоритм. На каждом шаге выбирает для каждой вершины лучший предикат среди всех возможных, разбивает по этому предикату объекты на две части. Дерево строим до выполнения критерия останова. \n",
    "\n",
    "#### Алгоритм:\n",
    "1. Поместить в корень всю выборку $R_1=X$\n",
    "2. Запустить построение из корня: рекурсивная функция SplitNode(1,  $R_1$)\n",
    "\n",
    "SplitNode(m,  $R_m$)\n",
    "1. Если выполнен критерий останова, то выход\n",
    "2. Ищем лучший предикат $j,t = argmin_{j,t} Q(R_m, j, t)$ Мы их ищем, перебирая все возможные варианты и выбирая тот, где наш критерий информативности дает максимальное значение, то есть, где уменьшение хаотичности как можно сильнее. \n",
    "3. Разбиваем его с помощью объекты\n",
    "$$R_l={(x,y) \\in R_m | [x_j<t]}, R_r={(x,y) \\in R_m | [x_j >=t]}$$\n",
    "4. Повторяем для дочерних вершин SplitNode(l, $R_l$), SplitNode(l, $R_r$)\n",
    "\n",
    "__Как построить максимально различные деревья?__\n",
    "\n",
    "Во время выбора предиката. Выбираем лучший предикат не из всех возможных, а из какого-то подмножества признаков. \n",
    "\n",
    "С рандомизацией по признакам: чем меньше признаков, тем ниже корреляция между ошибками деревьев. \n",
    "\n",
    "! Это не то же самое, что случайные подпространства!\n",
    "Там всё дерево строилось на подмножестве признаков и какой-то золотой признак мог не попасть. Здесь рандомизация идёт в отдельных вершинах. \n",
    "\n",
    "\n",
    "Рекомендованный размер количества признаков:\n",
    "- Регрессия $q=\\frac{d}{3}$\n",
    "- Классификация $q=\\sqrt(d)$\n",
    "\n",
    "#### Алгоритм случайного леса (random forest):\n",
    "1. Строим N деревьев независимо\n",
    "2. Для каждого дерева выбираем подвыборку обучающей выборки с помощью бутстрапа\n",
    "3. Строим решающее дерево пока в каждом листе не окажется не более n объектов (3-5, деревья очень глубокие)\n",
    "4. На каждой вершине выбираем лучший предикат только из подмножества признаков.\n",
    "\n",
    "\n",
    "\n",
    "Классификация:\n",
    "$$a_N(x)=argmax_{y \\in Y} \\sum _{n=1} ^N [b_n(x)=y]$$\n",
    "\n",
    "Регрессия:\n",
    "$$a_N(x) = \\frac {1}{N} \\sum _{n=1} ^N b_n(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как зависит ошибка случайного леса в зависимости от числа деревьев в нём. \n",
    "\n",
    "Три кривые - это ошибки на тестовой выборке, но при разном размере подмножества признаков, которые выбираются в кажлой вершине. \n",
    "\n",
    "Лучший вариант - корень.\n",
    "\n",
    "Хотя здесь растет количество деревьев и композиция становится сложнее, ошибка выходит на асимптоту и перестает расти. \n",
    "\n",
    "Смещение остается прежним, разброс не растет. \n",
    "<img src='images/ensembles10.png'>\n",
    "\n",
    "Эти свойства делают случайный лес самым универсальным методом машинного обучения. \n",
    "\n",
    "- практически нет гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Со случайным лесом можно использовать out-of-bag estimation.\n",
    "\n",
    "Каждое дерево обучается примерно на 63% данных. Остальные объеты можно сделать тестовой выборкой для дерева (можно оценить обобщающую способность без тестовой выборки)\n",
    "\n",
    "*жуткая формула*\n",
    "\n",
    "Для конкретного объекта учитываем только предсказания тех деревьев, для которых объект является тестовым. \n",
    "\n",
    "Можно оценивать важность признаков (перемешать признак, посмотреть, упало ли качество). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
