{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг\n",
    "## Бэггинг\n",
    "- В бэггинге строится несколько немного разных моделей и они усредняются\n",
    "\n",
    "- Смещение $a_N(x)$ такое же, как у $b_n(x)$ (сложность модели не увеличится -> в бэггинге в качестве базовых моделей нужно брать сложные модели, например, глубокие решающие деревья)\n",
    "\n",
    "- Разброс $a_N(x)$:\n",
    "$$\\frac{1}{N}(разброс b_n(x)) + ковариация(b_n(x), b_m(x)) $$\n",
    "\n",
    "- Если базовые модели независимы, то разброс уменьшается в N раз (независимость = их ошибки независимы. По тому, что первая модель ошиблась на первом объекте, нельзя сказать, что вторая модель тоже ошибется на нем)\n",
    "\n",
    "- Чем более похожи выходы базовых моделей, тем меньше эффект от построения композиции.\n",
    "\n",
    "\n",
    "__Следовательно, базовые модели должны быть сложными и как можно более независимыми (непохожими).__\n",
    "\n",
    "### Проблемы:\n",
    "- Если базовая модель окажется смещенной, то и композиция не справится задачей (мало данных, не можем построить дерево, которое использует все признаки)\n",
    "- Базовые модели долго обучать и применять, дорого хранить (в каждой вершине нужно перебрать все доступные предикаты, дерево глубокое - вершин много)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бустинг\n",
    "- Простые базовые модели (деревья глубины 1-4)\n",
    "- Построение композиции происходит последовательно и жадно\n",
    "- Каждая следующая модель будет строиться так, чтобы максимально корректировать ошибки построенных моделей\n",
    "- n+1 модель построим так, чтобы её добавление в композицию как можно сильнее уменьшило ошибку\n",
    "\n",
    "\n",
    "Базовая модель:\n",
    "$$a_N(x)=\\sum _{n=1} ^N b_n(x)$$\n",
    "Как в бэггинге, но без усреднения\n",
    "\n",
    "Обучение n-ой модели:\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l L(y_i, b_1(x_i)) -> min_{b_1(x)}$$\n",
    "Минимизируем функционал ошибки\n",
    "\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l L(y_i, a_{N-1}(x_i)+b_N(x_i)) -> min_{b_N(x)}$$\n",
    "\n",
    "Считаем функцию потерь от правильного ответа и суммы ансамбля предыдущих n-1 моделей и новой модели $b_n$\n",
    "\n",
    "\n",
    "Жадное построение - значит фиксируем все предыдущие модели, никак их не меняем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бустинг для MSE\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l (a_{N-1}(x_i)+b_N(x_i)-y_i)^2 -> min_{b_N(x)}$$\n",
    "\n",
    "Перегруппируем слагаемые\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l (b_N(x_i) - (y_i - a_{N-1}(x_i)))^2 -> min_{b_N(x)}$$\n",
    "\n",
    "$y_i - a_{N-1}(x_i)$ - фиксирована, так как алгоритм - жадный\n",
    "\n",
    "\n",
    "Задача выглядит как обучение новой модели на среднеквадратическую ошибку с новыми значениями целевых переменных. Обозначим как \n",
    "$$s_i ^{(N)} = y_i - a_{N-1}(x_i)$$.\n",
    "Это сдвиги или остатки\n",
    "\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l (b_N(x_i) - s_i ^{(N)})^2 -> min_{b_N(x)}$$\n",
    "\n",
    "\n",
    "В случае с MSE можно просто подменить целевую переменную\n",
    "\n",
    "\n",
    "Строим первое дерево (подгоняем под исходные целевые переменные)\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l (b_1(x_i) - y_i)^2 -> min_{b_1(x)}$$\n",
    "Второе дерево:\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l (b_2(x_i) - (y_i-b_1(x_i)))^2 -> min_{b_2(x)}$$\n",
    "Третья модель:\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l (b_3(x_i) - (y_i-b_1(x_i)-b_2(x_i)))^2 -> min_{b_3(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация\n",
    "регрессия\n",
    "\n",
    "\n",
    "<img src='images/ensembles11.png'>\n",
    "\n",
    "В качестве базового алгоритма берём decision stumps (решающие пни). Это деревья глубины 1, в которых есть только один предикат. В зависимости от его значения выдается один их двух ответов. \n",
    "\n",
    "<img src='images/ensembles12.png'>\n",
    "\n",
    "Слева визуализация самой модели, справа - остатки,\n",
    "то есть отклонения уже построенной композиции от истинных целевых переменных. \n",
    "1. видно, что остатки стали чуть лучше, чем исходная выборка. Например, верхние два облака точек выровнялись друг с другом. \n",
    "2. остатки становятся еще лучше, модель тоже становится сложнее и уже учитывает правое облако точек, выдавая на нем правильные ответы. \n",
    "<img src='images/ensembles13.png'>\n",
    "После 20 итерации остатки почти нулевые"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от случайного леса, где было усреднение независимых моделей, бустинг легко переобучается. \n",
    "\n",
    "\n",
    "<img src='images/ensembles14.png'>\n",
    "\n",
    "\n",
    "x - количество деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- В случае с MSE обучение базовых моделей сводится к обычной процедуре обучения с заменой целевой переменной\n",
    "- Бустинг может переобучаться, поэтому надо следить за ошибкой на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Произвольная функция потерь для бустинга\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l L(y_i, a_{N-1}(x_i)+b_N(x_i)) -> min_{b_N(x)}$$\n",
    "Если попробовать снова обучаться на остатки, как в MSE:\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l L(y_i-a_{N-1}(x_i),b_N(x_i)) -> min_{b_N(x)}$$\n",
    "Это плохая идея.\n",
    "Пример: логистическая функция потерь\n",
    "$$a_N(x)=sign \\sum_{n=1} ^N b_n(x)$$\n",
    "\n",
    "$$L(y,z)=log(1+exp(-yz))$$\n",
    "$y$-правильный ответ, $z$- прогноз модели, $yz$ имеет смысл отступа\n",
    "$$z=b_N(x_i)$$\n",
    "$$y=y_i-a_{N-1}(x_i)$$\n",
    "Если обучаться на остатки:\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l log(1+exp(-(y_i-a_{N-1}(x_i)b_N(x_i)) -> min_{b_N(x)}$$\n",
    "\n",
    "То не учитываются правильные ответы. Отсупы для корректировки неправильных ответов равноценны. \n",
    "\n",
    "Пример: MSLE\n",
    "- аргумент логарифма может стать отрицательным\n",
    "- объекты не равноценные\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Нельзя заменить обучение добавки к композиции на обучение базовой модели на отклонение от ответов\n",
    "- Не учитываются особенности функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг в общем виде\n",
    "Для произвольной дифференцируемой функции потерь\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l L(y_i, a_{N-1}(x_i)+b_N(x_i)) -> min_{b_N(x)}$$\n",
    "\n",
    "Как посчитать, куда и как сильно сдвигать $a_{N-1}(x_i)$, чтобы уменьшить ошибку?\n",
    "\n",
    "Можно посчитать производную:\n",
    "\n",
    "$$s_i^{(N)}=-\\frac{\\partial}{\\partial z}L(y_i, z)|z=a_{N-1}(x_i)$$\n",
    "\n",
    "Первый аргумент фиксирован, считаем по второму аргументу (по прогнозу). Берем значение этой производной в точке $z=a_{N-1}(x_i)$\n",
    "\n",
    "- Посчитаем значение производной в той точке, которая равна текущему прогнозу композиции в бустинге, и возьмем с минусом.\n",
    "\n",
    "- Знак производной будет показывать, в какую сторону нужно сдвигать уже построенную композицию, чтобы ошибка этой композиции уменьшилась.\n",
    "- Если эта производная будет отрицательной — значит, нужно уменьшать прогноз.\n",
    "- Если производная будет положительной (антипроизводная) — значит, нужно увеличивать прогноз.\n",
    "\n",
    "-Величина этой производной (ее абсолютное значение) будет показывать, насколько сильно мы уменьшим ошибку, если сдвинемся в этом направлении. Если производная будет большой по модулю, это будет означать, что, если мы даже немножко сдвинемся в эту сторону, ошибка уменьшится сильно. Если производная будет равна примерно нулю, это будет означать, что, даже если мы сдвинемся в ту сторону, ошибка, скорее всего, практически не поменяется.\n",
    "- То есть если антипроизводные (производные с минусом) будут близки к нулю, то можно на этом объекте ничего особо не менять, все равно ошибка уже не изменится.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный бустинг\n",
    "Обучение N-й модели:\n",
    "$$\\frac{1}{l} \\sum_{i=1} ^l (b_N(x_i)-s_i^{(N)})^2 -> min_{b_N(x)}$$\n",
    "\n",
    "Сдвиги:\n",
    "$$s_i^{(N)}=-\\frac{\\partial}{\\partial z}L(y_i, z)|z=a_{N-1}(x_i)$$\n",
    "\n",
    "Будем N-ю базовую модель $b_n(x)$ так, чтобы она минимизировала среднеквадратичную ошибку между прогнозами этой модели и сдвигами.\n",
    "\n",
    "- Как бы градиентный спуск в пространстве ответов на обучающей выборке.\n",
    "\n",
    "- мы знаем прогнозы нашей модели на всех объектах обучающей выборки, считаем производные по этим самым прогнозам, и эти производные в совокупности говорят, как нам сдвинуть прогнозы композиции с помощью новой модели, чтобы как можно сильнее уменьшить ошибку этой композиции. \n",
    "\n",
    "- Получается, что базовая модель будет делать корректировки в основном на тех объектах, где есть шанс сильно уменьшить ошибку за счет сдвига, а если сдвиги для каких-то объектов находятся около нуля, то мы и новую модель будем просить выдавать прогноз около нуля, то есть ничего не менять, потому что ошибку все равно уже не уменьшить.\n",
    "\n",
    "- Поскольку это производные по функции потерь (производные функции потерь по аргументам), то получается, что эта задача, обучение N-й модели, учитывает особенности исходной функции потерь. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Градиентный бустинг для MSE:\n",
    "<img src='images/ensembles15.png'>\n",
    "Получилась та же формула, что и выше\n",
    "\n",
    "\n",
    "#### Градиентный бустинг для логистической функции потерь:\n",
    "<img src='images/ensembles16.png'>\n",
    "- По формуле видно, что если отступ большой и положительный (уверены в правильном ответе), то дробь близка к нулю\n",
    "- Если же отступ большой и отрицательный (уверены в неправильном ответе), то дробь близка к +1 или -1\n",
    "\n",
    "\n",
    "В итоге:\n",
    "- Чтобы учесть особенности функции потерь, можно посчитать её производные в точке текущего прогноза композиции\n",
    "- Базовую модель будем обучать на эти производные (со знаком минус)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры\n",
    "- глубина базовых деревьев\n",
    "- число деревьев N\n",
    "\n",
    "### Проблемы бустинга\n",
    "- сдвиги показывают направление, в котором надо сдвинуть композицию на всех объектах обучающей выборки\n",
    "- базовые модели, как правило, очень простые\n",
    "- могут не справиться с приближением этого направления\n",
    "\n",
    "Чтобы решить эти проблемы, можно добавлять деревья в композицию с небольшим весом (шаг)\n",
    "\n",
    "### Рандомизация по признакам\n",
    "- Можно обучать деревья на случайных подмножествах признаков\n",
    "- Бустинг уменьшает смещение, поэтому итоговая композиция всё равно получится качественной\n",
    "- Может снизить переобучение\n",
    "\n",
    "### Регуляризация деревьев\n",
    "- введение длины шага и семплирование признаков\n",
    "- штрафы за число листьев в дереве\n",
    "- штрафы за величину прогнозов в листьях дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Базового алгоритма градиентного бустинга недостаточно для получения сильных результатов, поэтому было разработано большое число улучшений и имплементаций. Можно улучшать тип деревьев, способ построения деревьев, подходы к регуляризации, вносить модификации в процедуру обучения деревьев_\n",
    "\n",
    "## Имплементации градиентного бустинга \n",
    "- XGBoost\n",
    "- LightGBM (leaf-wise growth, поиск порогов на основе производных)\n",
    "- CatBoost (ODT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
