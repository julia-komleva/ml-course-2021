{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решающие деревья\n",
    "Ранее мы рассматривали метрические методы (метод k-ближайших соседей) и линейные методы классификации и регрессии.\n",
    "\n",
    "Линейные модели были дифференцируемыми, мы могли их усложнять (например, добавив полиномиальных признаков), и всё равно обучали их потом градиентными методами.\n",
    "\n",
    "Решающее дерево - пример недифференцируемых моделей. \n",
    "\n",
    "На практике отдельные решающие деревья используются редко. В проде используют композиции моделей: бэггинг, бустинг, стекинг и блендинг. Для этого берется несколько немного разных моделей и они как-то объединяются в одну мощную модель. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример\n",
    "В задаче предсказания стоимости квартиры по таким признакам, как площадь, этаж, район, расстояние до метро была сложность с признаком \"расстояние до метро\" (связь этого признака с целевой переменной была нелинейна). Приходилось разбивать признак на интервалы и добавлять признаки - индикаторы вхождения значения признака в интервал. Необходимость таких модификаций признака - недостаток линейных моделей.\n",
    "\n",
    "Вторая проблема - формула линейной модели предполагает независимость признаков (а её нет). Например, для богатого человека в квартире с большой площадью и личным автомобилем может быть не важна близость метро. \n",
    "$$a(x) = w_0 + w_1 *площадь + w_2 * этаж + w_3 * расстояние до метро + ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эту проблему можно исправить добавлением парных полиномиальных признаков.\n",
    "\n",
    "Линейная модель с полиномиальными признаками:\n",
    "\n",
    "$$a(x) = w_0 + w_1 *площадь + w_2 * этаж + w_3 * расстояние до метро + w_4 * площадь ^2  + w_5 * этаж ^2 + w_6 * расстояние до метро ^2 + w_7 * площадь * этаж ...$$\n",
    "\n",
    "Недостаток такой модели - её ответ сложно интерпретировать. Непонятно, что такое, например, $$расстояние до метро * этаж ^2$$\n",
    "\n",
    "Кроме того, такие модели может быть очень тяжело обучать. Если изначально было 10 признаков, то при добавлении полиномиальных признаков второй степени признаков будет 55. Третьей - 220, 4 - 715. Для моделей с таким количеством признаков и данных нужно очень много. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модифицируем признаки линейной модели для улучшения её интерпретируемости. \n",
    "$$a(x) = w_0 + w_1 * [30 < площадь < 50] + w_2 * [50 < площадь < 80] + ... w_{20} * [2 < этаж < 5] + ... + w_{100} * [30<площадь<50][2<этаж<5]+ ...$$\n",
    "\n",
    "Такие признаки интерпретируются гораздо проще, но их становится еще больше (у каждого признака несколько интервалов).\n",
    "\n",
    "В частности, если есть признак с большим весом и этот признак вычисляется как индикатор того, что площадь — от 30 до 50 квадратных метров, индикатор того, что этаж — от второго до пятого и расстояние до метро — от 100 до 500 метров, то легко объяснить заказчику модели, что если квартира удовлетворяет трем таким критериям, то данные показывают, что она будет стоить больше. \n",
    "\n",
    "Проблема с интерпретируемостью решена, но проблема с количеством признаков только усугубилась. \n",
    "\n",
    "Поэтому нужен какой-то другой способ искть нелинейные связи между признаками и целевой переменной. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решающее дерево для классификации\n",
    "Бинаризованные признаки \n",
    "$$[30<площадь<50][2<этаж<5][500 < расстояние до метро < 1000]$$ легко интерпретировать, они ловят нелинейные закономерности. Но таких признаков может быть слишком много, нужно выбрать только хорошие логические правила. И нужно научиться составлять модель из логических правил. \n",
    "\n",
    "Хороший подход - решающее дерево. \n",
    "\n",
    "\n",
    "<img src='images/dt1.png'>\n",
    "\n",
    "Сверху - корень, внизу - листья (прогнозы). В каждой внутренней вершине записано некоторое условие (предикат, $x_j < t$, признак меньше порога). \n",
    "\n",
    "<img src='images/dt2.jpeg'>\n",
    "\n",
    "<img src='images/dt3.webp'>\n",
    "\n",
    "<img src='images/dt4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример дерева решений на модельных данных. Бинарная классификация. Эту задачу не решить обычной линейной моделью. \n",
    "\n",
    "<img src='images/dt5.png'>\n",
    "\n",
    "В корне проверяется, что первый признак (по у) меньше 0.002. Пространство поделилось на 2 части вблизи нуля. \n",
    "\n",
    "Левый потомок - лист, если значение признака меньше 0, то сразу выдается ответ - синий класс. \n",
    "\n",
    "<img src='images/dt6.png'>\n",
    "\n",
    "Если оказались выше черты, то смотрим на нулевой признак (по х) и сравниваем с порогом 1.433. Если условие не выполнено (значение признака больше порога), то будет листовая вершина и ответ - синий. \n",
    "\n",
    "<img src='images/dt7.png'>\n",
    "\n",
    "<img src='images/dt8.png'>\n",
    "\n",
    "<img src='images/dt9.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая листовая вершина соответствует прямоугольной области, внутри которой выдается константное предсказание. \n",
    "\n",
    "<img src='images/dt10.png'>\n",
    "\n",
    "В этом дереве была ограничена глубина. \n",
    "\n",
    "Если не ограничивать глубину, то происходит переобучение:\n",
    "<img src='images/dt11.png'>\n",
    "Оба дерева решают задачу идеально. \n",
    "\n",
    "Но во втором дереве есть маленькие области, которые выделяют только по одному объекту. \n",
    "\n",
    "Решающее дерево можно строить до тех пор, пока каждый лист не будет соответствовать ровно одному объекту (но дерево может оказаться переобученным). \n",
    "\n",
    "Деревом можно идеально разделить любую выборку, если нет объектов с одинаковыми признаками, но разными ответами. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решающее дерево для регрессии\n",
    "Один признак - х. В корневой вершине сравниваем значение признака с порогом 3.133.\n",
    "\n",
    "\n",
    "<img src='images/dt12.png'>\n",
    "\n",
    "В корневой вершине сравниваем значение признака с порогом 3.133.\n",
    "\n",
    "\n",
    "<img src='images/dt13.png'>\n",
    "\n",
    "Слева одно значение:\n",
    "\n",
    "\n",
    "<img src='images/dt14.png'>\n",
    "\n",
    "\n",
    "Справа. В итоге произошло разделение на 4 области. В каждой области прогноз константный. \n",
    "\n",
    "\n",
    "<img src='images/dt15.png'>\n",
    "\n",
    "Переобученное дерево для регрессии:\n",
    "\n",
    "<img src='images/dt16.png'>\n",
    "\n",
    "#### Резюме:\n",
    "- Решающие деревья - комбинация простых логических правил\n",
    "- Деревья разбивают признаковое пространство на области, внутри каждой из областей предсказание константное\n",
    "- Дерево легко переобучить, если не ограничивать его глубину"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Структура решающих деревьев\n",
    "- Порог на признак $[x_j <t]$ - не единственный вариант\n",
    "- В предикат можно поместить линейную модель $[<w,x> < t]$, тогда область будет делиться не прямоугольниками\n",
    "- Предикат с метрикой $[\\rho(x, x_0)<t]$. Как в knn смотреть на расстояние от объекта $x$ до какого-то эталонного объекта $x_0$ и сравнивать это расстояние с порогом. Если метрика евклидова - круги. \n",
    "- Но в этом нет необходимости. Даже с простыми предикатами (порог на признак) можно построить хорошую модель, и это будет легче). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прогнозы в листьях\n",
    "- Константный прогноз $c_v \\in Y$\n",
    "- $Y$ - пространство объектов\n",
    "- $v$ - номер листовой вершины\n",
    "- $R_v$ - все объекты, которые попали в листовую вершину $v$\n",
    "- Предсказываем средний ответ на этих объектах\n",
    "\n",
    "\n",
    "Регрессия:\n",
    "$$c_v = \\frac{1}{|R_v|} \\sum _{(x_i, y_i) \\in R_v} y_i$$\n",
    "\n",
    "Классификация:\n",
    "- вычисляем самый популярный класс из $R_v$\n",
    "- $k$ - класс\n",
    "$$c_v = argmax_{k \\in Y} \\sum _{(x_i, y_i) \\in R_v} [y_i = k]$$\n",
    "\n",
    "- Если нужна вероятность класса, то можно взять долю объектов этого класса в данной вершине. Эти вероятности будут суммироваться в единицу по всем классам. \n",
    "\n",
    "$$c_{vk} = \\frac{1}{|R_v|} \\sum _{(x_i, y_i) \\in R_v} [y_i = k]$$\n",
    "\n",
    "- Можно усложнить листья и, например, в каждом листе прогнозировать линейной моделью\n",
    "$$c_v (x) = <w_v, x>$$\n",
    "\n",
    "#### Формула для всего дерева\n",
    "- Дерево разбивает признаковое пространство на области $R_1, ..., R_J$\n",
    "- Каждая область $R_j$ соответствует листу\n",
    "- В области $R_j$ прогноз $c_j$ константный\n",
    "$$a(x)=\\sum _{j=1} ^J c_j [x \\in R_j] $$\n",
    "\n",
    "- Можно интерпретировать как линейную модель с поиском хороших нелинейных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предикаты и критерии информативности\n",
    "\n",
    "Как выбирать предикаты:\n",
    "#### Жадное построение\n",
    "Есть вершина, в которой 6 объектов, три класса, нужно разбить на 2 части\n",
    "\n",
    "<img src='images/dt17.png'>\n",
    "\n",
    "Хорошее ли это разбиение? 2 вершины, в каждой по три класса\n",
    "\n",
    "<img src='images/dt18.png'>\n",
    "\n",
    "Лучше? \n",
    "\n",
    "<img src='images/dt19.png'>\n",
    "\n",
    "А так?\n",
    "\n",
    "<img src='images/dt20.png'>\n",
    "\n",
    "В итоге:\n",
    "\n",
    "<img src='images/dt21.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как сравнивать разбиения? Какое лучше?\n",
    "ЖЖСС и ЗЗ или ЖСЗ ЖСЗ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Энтропия\n",
    "Мера неопределенности исхода эксперимента\n",
    "\n",
    "<img src='images/dt22.png'>\n",
    "Где энтропия выше?\n",
    "\n",
    "В первом эксперименте было непонятно, что выпадет, вероятности одинаковые. После эксперимента - знаем ответ. Значит получено много информации. \n",
    "\n",
    "Во втором эксперименте есть варианты, которые выпадают очень редко, никогда и почти всегда. Уже до начала эксперимента можно предположить, какой вариант скорее всего выпадет. В результате эксперимента почти ничего не узнаем. \n",
    "\n",
    "В первом случае энтропия высокая, есть много неопределенности. \n",
    "\n",
    "Во втором случае энтропия низкая, мало неуверенности. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть дано распределение случайной величины с $n$ исходами. \n",
    "\n",
    "Вероятности исходов: $p_1, p_2, ... p_n$\n",
    "\n",
    "Энтропия распределения:\n",
    "\n",
    "$$H(p_1, ..., p_n) = - \\sum _{i=1} ^{n} p_i log_2 p_i$$\n",
    "\n",
    "- может получиться, что у какого-то исхода может быть вероятность, равная нулю. В этом случае мы получим слагаемое, равное нулю умножить на логарифм нуля\n",
    "- логарифм нуля — это минус бесконечность.\n",
    "- Но эта неопределенность равна нулю. Поскольку p стремится к нулю быстрее, чем логарифм p стремится к нулю, то их произведение в пределе будет равно нулю\n",
    "\n",
    "\n",
    "- Энтропия всегда будет неотрицательной\n",
    "- минимальное значение равно нулю, она будет равна нулю, если распределение имеет один исход с вероятностью единица\n",
    "- максимального значения энтропия будет достигать, если распределение равномерное, то есть если все $p_i$ равны.\n",
    "- максимальное значение, которое будет достигаться при равных $p_i$, зависит от того, чему равно n: при разных n будет разное максимальное значение энтропии\n",
    "\n",
    "\n",
    "Для дерева:\n",
    "- Число исходов в эксперименте - это число классов $K$\n",
    "- Вероятность класса - доля соответствующих объектов в вершине\n",
    "\n",
    "$$p_k=\\frac{1}{|R|} \\sum _{(x_i, y_i) \\in R} [y_i=k]$$\n",
    "\n",
    "- Нулевая энтропия - в вершине только один класс\n",
    "- Максимальная энтропия - поровну объектов каждого класса\n",
    "\n",
    "<img src='images/dt23.png'>\n",
    "\n",
    "Суммируем энтропии по двум вершинам:\n",
    "- (0.5, 0.5, 0) и (0, 0, 1) -> 0.693 + 0 = 0.693\n",
    "- (0.33, 0.33, 0.33) и (0.33, 0.33, 0.33) -> 1.09 + 1.09 = 2.18\n",
    "\n",
    "Первое разбиение лучше (энтропия ниже)\n",
    "\n",
    "#### Резюме\n",
    "- Дерево можно строить путём последовательного разбиения\n",
    "- В задачах классификации можно выбирать разбиение, при котором в дочерних вершинах разнообразие классов как можно ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерии информативности\n",
    "Энтропия - характеристика \"хаотичности\" вершины (impurity)\n",
    "\n",
    "\"хаотичность\" можно измерить и другими способами\n",
    "\n",
    "### Критерий Джини\n",
    "$$H(p_1, ..., p_K) = \\sum ^K _{i=1} p_i(1-p_i)$$\n",
    "\n",
    "- берем вероятности классов в вершине $p_1, ..., p_K$, но вместо энтропии считаем сумму \n",
    "- Вероятность ошибки случайного классификатора, который выдает класс $k$ с вероятностью $p_k$\n",
    "- Примерно пропорционально количеству пар объектов, относящихся к разным классам\n",
    "\n",
    "<img src='images/dt24.png'>\n",
    "\n",
    "- у критерия Джини максимум — это 0.5, у энтропийного максимум — 1, но по форме они абсолютно одинаковые, то есть неважно, какой критерий брать: и тот и тот поощряет, чтобы у нас распределение было вырожденное, и тот и тот штрафует максимально за равномерное распределение; и между ними как-то он интерполирует.\n",
    "\n",
    "- критерий Джини будет немного быстрее, потому что в энтропийном критерии нужно брать логарифм, а в критерии Джини просто просуммировать квадраты. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как понять, какой предикат лучше?\n",
    "\n",
    "\n",
    "Нужно сравнить хаотичность в исходной вершине и в двух дочерних\n",
    "\n",
    "<img src='images/dt25.png'>\n",
    "\n",
    "$$Q(R, j, t) = H(R) - H(R_l) - H(R_r) -> max_{j, t}$$\n",
    "- $R_l$ - объекты, которые попадают в левое поддерево\n",
    "- $R_r$ - объекты, которые попадают в правое поддерево\n",
    "- $j$ - признак, по которому разбиваем\n",
    "- $t$ - порог\n",
    "\n",
    "Или так:\n",
    "$$Q(R, j, t) = H(R_l) + H(R_r) -> min_{j, t}$$\n",
    "\n",
    "Проблема:\n",
    "\n",
    "<img src='images/dt26.png'>\n",
    "\n",
    "(5/6, 1/6) и (1/6, 5/6)-> 0.65 + 0.65 = 1.3\n",
    "\n",
    "(6/11, 5/11) и (0, 1) -> 0.994 + 0 = 0.994 Лучше по числу, но приводит к переобучению (когда в листе один объект)\n",
    "\n",
    "Добавим штраф:\n",
    "\n",
    "$$Q(R, j, t) = H(R) - \\frac {|R_l|}{|R|}H(R_l) - \\frac {|R_r|}{|R|}H(R_r) -> max_{j,t}$$\n",
    "\n",
    "или\n",
    "\n",
    "$$Q(R, j, t) = \\frac {|R_l|}{|R|}H(R_l) + \\frac {|R_r|}{|R|}H(R_r) -> min_{j,t}$$\n",
    "\n",
    "- Хаотичность левой вершины домножается на долю объектов, которая ушла влево\n",
    "- Хаотичность правой вершины домножается на долю объектов, которая ушла вправо\n",
    "\n",
    "- (5/6, 1/6) и (1/6, 5/6)-> 0.5 * 0.65 + 0.5* 0.65 = 0.65\n",
    "\n",
    "- (6/11, 5/11) и (0, 1) -> 11/12 * 0.994 + 1/12 * 0 = 0.911\n",
    "\n",
    "Для регрессии:\n",
    "<img src='images/dt27.png'>\n",
    "\n",
    "<img src='images/dt28.png'>\n",
    "\n",
    "Во втором варианте меньше разброс\n",
    "\n",
    "$$H(R) = \\frac{1}{|R|} \\sum _{(x_i, y_i) \\in R} (y_i - y_R)^2$$\n",
    "$$y_R = \\frac{1}{|R|} \\sum _{(x_i, y_i) \\in R} y_i$$\n",
    "\n",
    "Хаотичность вершины можно измерять дисперсией ответов в ней\n",
    "\n",
    "#### Резюме\n",
    "- Предикат можно выбирать так, чтобы он как можно сильнее уменьшал \"хаотичность\" вершин\n",
    "- Качество предиката измеряется с помощью критерия информативности\n",
    "- Много вариантов для критериев информативности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Жадное построение дерева\n",
    "- Оптимальный вариант - перебрать все возможные деревья, выбрать самое маленькое среди безошибочных. Слишком долго. \n",
    "- Поэтому будем строить жадно\n",
    "\n",
    "- Мы уже знаем, как выбрать лучший предикат для разбиения вершины\n",
    "- Начнем с корня дерева, будем разбивать последовательно, пока не выполнится некоторый критерий останова\n",
    "\n",
    "Критерий останова:\n",
    "- Ограничить глубину\n",
    "- Ограничить количество листьев\n",
    "- Задать минимальное число объектов в вершине\n",
    "- Задать минимальное уменьшение хаотичности при разбиении\n",
    "\n",
    "#### Алгоритм:\n",
    "1. Поместить в корень всю выборку $R_1=X$\n",
    "2. Запустить построение из корня: рекурсивная функция SplitNode(1,  $R_1$)\n",
    "\n",
    "SplitNode(m,  $R_m$)\n",
    "1. Если выполнен критерий останова, то выход\n",
    "2. Ищем лучший предикат $j,t = argmin_{j,t} Q(R_m, j, t)$ Мы их ищем, перебирая все возможные варианты и выбирая тот, где наш критерий информативности дает максимальное значение, то есть, где уменьшение хаотичности как можно сильнее. \n",
    "3. Разбиваем его с помощью объекты\n",
    "$$R_l={(x,y) \\in R_m | [x_j<t]}, R_r={(x,y) \\in R_m | [x_j >=t]}$$\n",
    "4. Повторяем для дочерних вершин SplitNode(l, $R_l$), SplitNode(l, $R_r$)\n",
    "\n",
    "#### Резюме:\n",
    "- Деревья строятся жадно, на каждом шаге вершина разбивается на две с помощью лучшего из предикатов\n",
    "- Алгоритм сложный и требует перебора всех предикатов на каждом шаге\n",
    "- Жадность построения заключается в том, что если какое-то разбиение сделано, то оно уже не может быть изменено"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "Реализовать вычисление энтропии и посчитать её значение для: \n",
    "- (0.2, 0.2, 0.2, 0.2, 0.2) \n",
    "- (0.5, 0.5) \n",
    "- (0.9, 0.05, 0.05, 0) \n",
    "- (0, 0, 0, 0, 1, 0)\n",
    "\n",
    "Объяснить полученные результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "\n",
    "- Напишите класс дерева, который реализует метод train, получающий на вход матрицу данных (𝑥) и целевые переменные (𝑦), на выходе функция возвращает дерево в виде словаря\n",
    "- в каждом случае записывается номер признака (𝑖), порог разбиения(𝑡), значение в листовой вершине слева (𝑦𝑙) и значение в листовой вершине справа (𝑦𝑟). \n",
    "- Вместо значения в листовой вершине может содержаться следующий словарь с теми же ключами и так далее\n",
    "\n",
    "Необходимо учесть:\n",
    "\n",
    "- Возможность строить дерево строго определенной глубины и возможность указать максимальное количество объектов в листовой вершине.\n",
    "- Пороги разбиения должны лежать строго по середине между ближайшими обектами.\n",
    "- Разбиений не требуется, если в получившемся множестве находятся объекты одного класса.\n",
    "- Количество различных классов объектов в целевой переменной может быть больше двух.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, depth=1, mex_num_leaf=1):\n",
    "        pass\n",
    "    def train(x, y):\n",
    "        pass\n",
    "    def predict(x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Постройте решающее дерево с глубиной 1, 2 и 3.\n",
    "- Также постройте дерево максимальной глубины\n",
    "- Визуализируйте получившиеся результаты классификатора на плоскости (код есть в предыдущих лабах)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x, y = make_moons(n_samples=100, noise=0.1)\n",
    "plt.scatter(x[:,0], x[:,1], c = y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3\n",
    "Выполните задание 2 с помощью sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
